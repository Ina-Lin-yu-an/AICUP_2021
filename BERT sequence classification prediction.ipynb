{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "dfselect = pd.read_csv('select_name.csv') \n",
    "# this file only contains 12470 files which was filtered by cosine similarity\n",
    "# you can replace with your only csv with (1) the article and (2) the word list after cut word\n",
    "dflabel = pd.read_csv('TrainLabel.csv')\n",
    "\n",
    "w2vmodel = Word2Vec.load('w2v50.model') # load the cut word w2v model to find the index of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb0498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in next sentence prediction model, the relative sentence is labeled with 0, \n",
    "# not relative sentence is labeled with 1\n",
    "# use the same label in classification\n",
    "\n",
    "labeltable = np.ones((1402, 1402))\n",
    "for i in range(len(dflabel)):\n",
    "    labeltable[dflabel.iloc[i,0], dflabel.iloc[i,1]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03553c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the ids instead of using the tokenizer of BERT\n",
    "# cut the words first, and replace the words with the index in the word dictionary \n",
    "# the dictionary contains all words in both Train dataset and Public dataset\n",
    "\n",
    "Path = './Train_only/'\n",
    "filelist = os.listdir(Path)\n",
    "\n",
    "sentence = []; label = []\n",
    "maxl = 0\n",
    "for i in range(len(dfselect)):\n",
    "    label.append(labeltable[dfselect.iloc[i,0], dfselect.iloc[i,1]])\n",
    "    temp = [101] # the start of the sentence\n",
    "    for j in range(0,2):\n",
    "        f = np.load(Path+str(dfselect.iloc[i,j]).zfill(4)+'.npz')\n",
    "        data1= list(f['text'])\n",
    "        for word in data1:\n",
    "            if len(word) > 1 and (w2vmodel.wv.index_to_key.index(word)+105 not in temp):\n",
    "                temp.append(w2vmodel.wv.index_to_key.index(word)+105)\n",
    "        temp = temp+[102] # the end of the each sentence\n",
    "        if len(temp)>256 and j==0: # if the first sentence is too long, cut it!\n",
    "            print(j, len(temp))\n",
    "            temp = temp[:256]+[102] \n",
    "            \n",
    "    if len(temp) > 512: # if the second sentence is too long, cut it!\n",
    "        print(len(temp))\n",
    "        temp = temp[0:511]+[102]\n",
    "        \n",
    "    sentence.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae5dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a12751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN=512\n",
    "\n",
    "input_ids=pad_sequences(sentence, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(\"Padding 第一個句子:\",input_ids[0])\n",
    "#建立mask\n",
    "attention_mask = []\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_mask.append(seq_mask)\n",
    "    \n",
    "print(\"第一個attention mask:\",attention_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8518bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('BertClassTrain', ids = input_ids, l = label, mask = attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6163cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the steps to deal with unbalence problem\n",
    "\n",
    "dflabel = pd.read_csv('TrainLabel.csv')\n",
    "dfselect = pd.read_csv('TrainLabel.csv')\n",
    "Path = './Train_only/'\n",
    "filelist = os.listdir(Path)\n",
    "\n",
    "sentence = []; label = []\n",
    "maxl = 0\n",
    "for i in range(len(dfselect)):\n",
    "    label.append(labeltable[dfselect.iloc[i,0], dfselect.iloc[i,1]])\n",
    "    temp = [101] # the start of the sentence\n",
    "    for j in range(0,2):\n",
    "        f = np.load(Path+str(dfselect.iloc[i,j]).zfill(4)+'.npz')\n",
    "        data1= list(f['text'])\n",
    "        for word in data1:\n",
    "            if len(word) > 1 and (w2vmodel.wv.index_to_key.index(word)+105 not in temp):\n",
    "                temp.append(w2vmodel.wv.index_to_key.index(word)+105)\n",
    "        temp = temp+[102] # the end of the each sentence\n",
    "        if len(temp)>256 and j==0: # if the first sentence is too long, cut it!\n",
    "            print(j, len(temp))\n",
    "            temp = temp[:256]+[102] \n",
    "            \n",
    "    if len(temp) > 512: # if the second sentence is too long, cut it!\n",
    "        print(len(temp))\n",
    "        temp = temp[0:511]+[102]\n",
    "        \n",
    "    sentence.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa842d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the relative data to balance the non-relative data\n",
    "# 1382*(2^3) = 1381*8 = 11048\n",
    "for i in range(0, 3):\n",
    "    sentence += sentence\n",
    "    label += label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21df8ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f81c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN=512\n",
    "\n",
    "input_ids=pad_sequences(sentence, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(\"Padding 第一個句子:\",input_ids[0])\n",
    "#建立mask\n",
    "attention_mask = []\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_mask.append(seq_mask)\n",
    "    \n",
    "print(\"第一個attention mask:\",attention_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e44af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('BertClassTrainTrue', ids = input_ids, l = label, mask = attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from transformers import BertForNextSentencePrediction, BertForSequenceClassification\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese')\n",
    "# model = torch.load('bertNextSeq1204_20.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad6ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "f = np.load('BertClassTrain.npz')\n",
    "input_ids1 = f['ids']\n",
    "label1 = f['l']\n",
    "attention_mask1 = f['mask']\n",
    "\n",
    "f = np.load('BertClassTrainTrue.npz')\n",
    "input_ids = f['ids']\n",
    "label = f['l']\n",
    "attention_mask = f['mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b8121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8afbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate data and shuffle it\n",
    "\n",
    "import random\n",
    "\n",
    "index = list(range(0,len(label)+len(label1)))\n",
    "random.shuffle(index)\n",
    "sinput_ids = np.concatenate((input_ids1, input_ids), axis=0)\n",
    "slabel = np.concatenate((label1, label), axis=0)\n",
    "sattention_mask = np.concatenate((attention_mask1, attention_mask), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b297cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc2541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Batsize = 4 # depends on your gpu memory\n",
    "\n",
    "train_inputs = torch.tensor(sinput_ids[index[3200:]])\n",
    "validation_inputs = torch.tensor(sinput_ids[index[:3200]])\n",
    "\n",
    "train_labels = torch.tensor(slabel[index[3200:]], dtype=torch.long)\n",
    "validation_labels = torch.tensor(slabel[index[:3200]], dtype=torch.long)\n",
    "\n",
    "train_masks = torch.tensor(sattention_mask[index[3200:]])\n",
    "validation_masks = torch.tensor(sattention_mask[index[:3200]])\n",
    "\n",
    "print(np.shape(train_inputs), np.shape(train_labels), np.shape(train_masks))\n",
    "print(np.shape(validation_inputs), np.shape(validation_labels), np.shape(validation_masks))\n",
    "\n",
    "train_data1 = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler1 = RandomSampler(train_data1)\n",
    "train_dataloader1 = DataLoader(train_data1, sampler=train_sampler1, batch_size=Batsize)\n",
    "\n",
    "va_data1 = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "va_sampler1 = RandomSampler(va_data1)\n",
    "va_dataloader1 = DataLoader(va_data1, sampler=va_sampler1, batch_size=Batsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7f66a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce412017",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31672895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "l1loss = torch.nn.L1Loss()\n",
    "\n",
    "acc_train = []\n",
    "epochs = 100\n",
    "model.train().cuda()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0; acc = 0; va_avg_loss = 0; va_acc = 0;\n",
    "    \n",
    "    for step, batch1 in enumerate(train_dataloader1): \n",
    "        batch1 = tuple(t.cuda() for t in batch1)\n",
    "        b_input_ids1, b_input_mask1, b_labels1 = batch1\n",
    "        optimizer.zero_grad()\n",
    "        cross, logit = model(b_input_ids1, token_type_ids=None, attention_mask=b_input_mask1, labels=b_labels1)[0:2]\n",
    "        acc += l1loss(torch.argmax(logit, dim=1).float(), b_labels1)\n",
    "        cross.backward()        # retain_graph=True \n",
    "        optimizer.step()\n",
    "\n",
    "        print(step, 'Inst_Loss: ', '%.6f' % float(cross), ', ACC: ', '%d' % float(acc), end = '\\r')\n",
    "        avg_loss += float(cross)\n",
    "    \n",
    "    acc_train.append(acc)    \n",
    "    now = time.asctime(time.localtime(time.time()))  \n",
    "    print('Epoch:', '%3d' % epoch, ', AVG loss:', '%.6f' % avg_loss , ', ACC:', '%d' % float(acc),', Time:', now)    \n",
    "    \n",
    "    model.eval()\n",
    "    for step, batch1 in enumerate(va_dataloader1): \n",
    "        batch1 = tuple(t.cuda() for t in batch1)\n",
    "        b_input_ids1, b_input_mask1, b_labels1 = batch1\n",
    "        cross, logit = model(b_input_ids1, token_type_ids=None, attention_mask=b_input_mask1, labels=b_labels1)[0:2]\n",
    "        va_acc += l1loss(torch.argmax(logit, dim=1).float(), b_labels1)\n",
    "\n",
    "        print(step, 'va_Loss: ', '%.6f' % float(cross), ', ACC: ', '%d' % float(va_acc), end = '\\r')\n",
    "        va_avg_loss += float(cross)\n",
    "    model.train()\n",
    "    \n",
    "    now = time.asctime(time.localtime(time.time()))  \n",
    "    print('Epoch:', '%3d' % epoch, ', AVG VA loss:', '%.6f' % va_avg_loss , ', ACC:', '%d' % float(va_acc),', Time:', now)\n",
    "    \n",
    "    if (epoch+1)% 5 == 0:\n",
    "        torch.save(model, 'BertSequenceClassification1206_'+str(epoch+1).zfill(2)+'.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97815223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please restart the kernel \n",
    "# Use the trained model to predict\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "epoch = 25# which epoch you want to use\n",
    "model = torch.load('BertSequenceClassification1206_'+str(epoch)+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c8e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "size = 50\n",
    "\n",
    "dfselect = pd.read_csv('public_select1204.csv') # this file contain 17343 data which filtered by cosine similarity\n",
    "\n",
    "w2vmodel = Word2Vec.load('w2v'+str(size)+'.model')\n",
    "\n",
    "Path = './Public_only/'\n",
    "filelist = os.listdir(Path)\n",
    "\n",
    "sentence = []; label = []\n",
    "\n",
    "for i in range(len(dfselect)):\n",
    "    temp = [101]\n",
    "    for j in range(0,2):\n",
    "        f = np.load(Path+str(dfselect.iloc[i,j]).zfill(4)+'.npz')\n",
    "        data1= list(f['text'])\n",
    "        for word in data1:\n",
    "            if len(word) > 1 and (w2vmodel.wv.index_to_key.index(word)+105 not in temp):\n",
    "                temp.append(w2vmodel.wv.index_to_key.index(word)+105)\n",
    "        temp = temp+[102]\n",
    "        if len(temp)>256 and j==0:\n",
    "            temp = temp[:256]+[102]\n",
    "    \n",
    "    if len(temp) > 512:\n",
    "        temp = temp[0:511]+[102]\n",
    "    sentence.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d10de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN=512\n",
    "\n",
    "input_ids=pad_sequences(sentence, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(\"Padding 第一個句子:\",input_ids[0][:256])\n",
    "#建立mask\n",
    "attention_mask = []\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_mask.append(seq_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a119b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('ClassPublic', ids = input_ids, mask = attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b1019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "f = np.load('ClassPublic.npz')\n",
    "input_ids = f['ids']\n",
    "attention_mask = f['mask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7988f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "inputs = torch.tensor(input_ids)\n",
    "masks = torch.tensor(attention_mask)\n",
    "print(np.shape(inputs), np.shape(masks))\n",
    "\n",
    "public_data1 = TensorDataset(inputs, masks)\n",
    "dataloader1 = DataLoader(public_data1, shuffle=False, batch_size=1) \n",
    "# prediction 1 by 1, and do not shuffle, otherwise we will miss the order in the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f2115",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "ans = []; i = 0;\n",
    "for step, batch1 in enumerate(dataloader1): \n",
    "    batch1 = tuple(t.cuda() for t in batch1)\n",
    "    b_input_ids1, b_input_mask1= batch1\n",
    "    out = model(b_input_ids1, token_type_ids=None, attention_mask=b_input_mask1)[0]\n",
    "    i+=1\n",
    "    if out[0][0] > out[0][1]:\n",
    "        ans.append(i)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d68ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfselect.iloc[ans].to_csv('1206_class.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
